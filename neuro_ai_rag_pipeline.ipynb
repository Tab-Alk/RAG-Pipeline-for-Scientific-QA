{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e87961d",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913f882",
   "metadata": {},
   "source": [
    "## Install Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd659e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pypdf langchain_community langchain-huggingface faiss-cpu sentence-transformers transformers torch accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cf66c6",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2db94e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fffcfac",
   "metadata": {},
   "source": [
    "## Define Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b49b929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_embeddings = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "device_embeddings = 'mps'\n",
    "\n",
    "model_kwargs_embeddings = {'device': device_embeddings}\n",
    "encode_kwargs_embeddings = {'normalize_embeddings': False}\n",
    "\n",
    "try:\n",
    "    embeddings_model = HuggingFaceEmbeddings(\n",
    "        model_name=model_name_embeddings,\n",
    "        model_kwargs=model_kwargs_embeddings,\n",
    "        encode_kwargs=encode_kwargs_embeddings\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing embeddings model: {e}\")\n",
    "    embeddings_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ebebd",
   "metadata": {},
   "source": [
    "##  Define Language Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c674ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "model_id_llm = \"google/flan-t5-base\"\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id_llm)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id_llm)\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        device_llm_pipeline = 'mps'\n",
    "    elif torch.cuda.is_available():\n",
    "        device_llm_pipeline = 0\n",
    "    else:\n",
    "        device_llm_pipeline = -1\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text2text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=512,\n",
    "        device=device_llm_pipeline\n",
    "    )\n",
    "    \n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERROR initializing LLM: {e}\")\n",
    "    llm = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7a6c5",
   "metadata": {},
   "source": [
    "## Parse and Load the PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8d2c3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'neuro_ai_research.pdf' loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "pdf_file_path = \"neuro_ai_research.pdf\"\n",
    "loader = PyPDFLoader(pdf_file_path)\n",
    "\n",
    "try:\n",
    "    pages = loader.load_and_split()\n",
    "    print(f\"File '{pdf_file_path}' loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"PDF file not found at '{pdf_file_path}'\")\n",
    "    pages = None\n",
    "except Exception as e:\n",
    "    print(f\"Could not load PDF. {e}\")\n",
    "    pages = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916c50df",
   "metadata": {},
   "source": [
    "## Create FAISS Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73600b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index saved to 'neuro_ai_faiss_embeddings' successfully.\n"
     ]
    }
   ],
   "source": [
    "faiss_index = None\n",
    "faiss_index_path = \"neuro_ai_faiss_embeddings\"\n",
    "\n",
    "if pages and embeddings_model:\n",
    "    try:\n",
    "        faiss_index = FAISS.from_documents(pages, embeddings_model)\n",
    "        faiss_index.save_local(faiss_index_path)\n",
    "        print(f\"FAISS index saved to '{faiss_index_path}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR creating/saving FAISS index: {e}\")\n",
    "else:\n",
    "    print(\"PDF pages or embeddings model not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a6841",
   "metadata": {},
   "source": [
    "## Load Saved FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62feb716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded from 'neuro_ai_faiss_embeddings' successfully.\n"
     ]
    }
   ],
   "source": [
    "loaded_faiss_index = None\n",
    "\n",
    "if embeddings_model:\n",
    "    try:\n",
    "        if os.path.exists(faiss_index_path):\n",
    "            loaded_faiss_index = FAISS.load_local(\n",
    "                faiss_index_path,\n",
    "                embeddings_model,\n",
    "                allow_dangerous_deserialization=True\n",
    "            )\n",
    "            print(f\"FAISS index loaded from '{faiss_index_path}' successfully.\")\n",
    "        elif faiss_index:\n",
    "            loaded_faiss_index = faiss_index\n",
    "            print(\"FAISS index from loaded successfully.\")\n",
    "        else:\n",
    "            print(f\"FAISS index not found at '{faiss_index_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR loading FAISS index: {e}\")\n",
    "else:\n",
    "    print(\"Cannot load FAISS index.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee965c",
   "metadata": {},
   "source": [
    "## Create the Conversational Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea4795e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversational Retrieval Chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "qa_chain = None\n",
    "\n",
    "if loaded_faiss_index and llm:\n",
    "    retriever = loaded_faiss_index.as_retriever(search_kwargs={'k': 3})\n",
    "    qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    print(\"Conversational Retrieval Chain created successfully.\")\n",
    "else:\n",
    "    print(\"Either the FAISS index or the LLM is not available.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c406dc",
   "metadata": {},
   "source": [
    "## Define a Function to Interact with the RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49916547",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "def ask_rag(query: str):\n",
    "    if not qa_chain:\n",
    "        return \"Sorry, the RAG chain is not initialized yet. Please make sure everything is set up before asking questions.\"\n",
    "    \n",
    "    if not query:\n",
    "        return \"It looks like you didn’t enter a question. Please provide a query to continue.\"\n",
    "\n",
    "    chain_input = {\"question\": query, \"chat_history\": chat_history}\n",
    "\n",
    "    print(f\"\\n[QUERY] Asking: {query}\")\n",
    "    \n",
    "    try:\n",
    "        response = qa_chain.invoke(chain_input)\n",
    "    except Exception as e:\n",
    "        return f\"[ERROR] Something went wrong while processing your query: {e}\"\n",
    "    \n",
    "    chat_history.append((query, response['answer']))\n",
    "\n",
    "    if response.get('source_documents'):\n",
    "        print(\"\\n[SOURCES] Retrieved the following source documents:\")\n",
    "        for i, doc in enumerate(response['source_documents']):\n",
    "            content_preview = doc.page_content[:250] + \"...\" if len(doc.page_content) > 250 else doc.page_content\n",
    "            source_page = doc.metadata.get('page', 'N/A')\n",
    "            print(f\"- Doc {i+1} (Page {source_page}): {content_preview}\\n\")\n",
    "    \n",
    "    return response['answer'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e080b40b",
   "metadata": {},
   "source": [
    "## Example Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9949804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q&A \n",
      "\n",
      "\n",
      "[QUERY] Asking: How are biological neural networks similar to Transformers?\n",
      "\n",
      "[SOURCES] Retrieved the following source documents:\n",
      "- Doc 1 (Page 13): From 1, we see that V (t) is dependent on the conductance,\n",
      "gL , of the resistor, the capacitance, C, of the capacitor, on the\n",
      "resting voltage (E L ) and of a current source I (t). If we\n",
      "multiply 1 by R := 1\n",
      "C , we obtain dvmem\n",
      "dt in terms of the\n",
      "memb...\n",
      "\n",
      "- Doc 2 (Page 14): J. D. Nuneset al.: Spiking Neural Networks: Survey\n",
      "the synaptic weights, W. One of the prevailing methods is\n",
      "the biologically inspired STDP. STDP results from a set of\n",
      "neurobiological ﬁndings that started in 1949, with Donald\n",
      "Hebb, who proposed a fun...\n",
      "\n",
      "- Doc 3 (Page 25): residual learning in SNNs. Much like Spiking ResNet [146],\n",
      "SEW ResNet substitutes the ReLU activation for a Spiking\n",
      "Neuron (SN), however, it also ﬁnds an element-wise func-\n",
      "tion, g, to realize identity mapping. This strategy overcomes\n",
      "the drawbacks o...\n",
      "\n",
      "\n",
      "[Answer 1] \n",
      "\n",
      "[QUERY] Asking: How are biological neurons different from Transformer neurons?\n",
      "\n",
      "[SOURCES] Retrieved the following source documents:\n",
      "- Doc 1 (Page 13): From 1, we see that V (t) is dependent on the conductance,\n",
      "gL , of the resistor, the capacitance, C, of the capacitor, on the\n",
      "resting voltage (E L ) and of a current source I (t). If we\n",
      "multiply 1 by R := 1\n",
      "C , we obtain dvmem\n",
      "dt in terms of the\n",
      "memb...\n",
      "\n",
      "- Doc 2 (Page 14): J. D. Nuneset al.: Spiking Neural Networks: Survey\n",
      "the synaptic weights, W. One of the prevailing methods is\n",
      "the biologically inspired STDP. STDP results from a set of\n",
      "neurobiological ﬁndings that started in 1949, with Donald\n",
      "Hebb, who proposed a fun...\n",
      "\n",
      "- Doc 3 (Page 25): residual learning in SNNs. Much like Spiking ResNet [146],\n",
      "SEW ResNet substitutes the ReLU activation for a Spiking\n",
      "Neuron (SN), however, it also ﬁnds an element-wise func-\n",
      "tion, g, to realize identity mapping. This strategy overcomes\n",
      "the drawbacks o...\n",
      "\n",
      "\n",
      "[Answer 2] \n"
     ]
    }
   ],
   "source": [
    "if qa_chain:\n",
    "    print(\"\\nQ&A \\n\")\n",
    "\n",
    "    # Example 1\n",
    "    question1 = \"How are biological neural networks similar to Transformers?\"\n",
    "    answer1 = ask_rag(question1)\n",
    "    print(f\"\\n[Answer 1] {answer1}\")\n",
    "\n",
    "    # Example 2\n",
    "    question2 = \"How are biological neurons different from Transformer neurons?\"\n",
    "    answer2 = ask_rag(question2)\n",
    "    print(f\"\\n[Answer 2] {answer2}\")\n",
    "\n",
    "else:\n",
    "    print(\"The RAG chain is not initialized.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00cea9-94f2-4dc5-b6e8-60dcb048677e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
